# -*- coding: utf-8 -*-
"""Llama-AI-whole

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/198oZR2dBT7Ijb4haYJP3oT11JbwwcS5l
"""

!pip install --upgrade transformers

!pip install transformers torch accelerate
!huggingface-cli login
!huggingface-cli whoami

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, StoppingCriteria, StoppingCriteriaList
from accelerate import infer_auto_device_map, init_empty_weights, load_checkpoint_and_dispatch
import torch
import json

model_name = "meta-llama/Llama-2-7b-chat-hf"
HF_Token = "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"

# Load the Llama-2 model and tokenizer with the token
tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=HF_Token)
model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=HF_Token)

# Initialize the pipeline
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# Define the prompt for Argument from Definition

scheme = "Argument from Definition"
stance = "pro"
topic = "Should AI be used to make hiring and promotion decisions?"
argument_pattern = "AI is X, because AI is Y"

"""**Similarity**"""

argument_similarity = "AI is similar to human decision-makers because both can make decisions based on data and patterns. Therefore, what is true for human decision-makers is likely true for AI in the context of hiring and promotion decisions."
prompt2 = (
    f"Generate an argument using the {scheme} approach, focusing on a {stance} perspective for the topic {topic}."
    f"Ensure the argument supports a pro stance exclusively, without any opposing viewpoints."
    f"The argument should be structured according to this format: {argument_similarity}"
)

# Generate the argument
outputs = generator(prompt2, max_length=250, temperature=0.9, num_return_sequences=1, do_sample=True, truncation=True)
generated_text = outputs[0]["generated_text"]

# Print the output
print(outputs[0]["generated_text"])
# Save the output to a text file
with open("Llama2-similarity-AI.txt", "w") as file:
    file.write(generated_text)

"""**Criterion**"""

argument_criterion = "AI is trustworthy, because it consistently produces unbiased decisions."
prompt3 = (
    f"Generate an argument using the {scheme} approach, focusing on a {stance} perspective for the topic {topic}."
    f"Ensure the argument supports a pro stance exclusively, without any opposing viewpoints."
    f"The argument should be structured according to this format: {argument_criterion}"
)

# Generate the argument
outputs = generator(prompt3, max_length=250, temperature=0.9, num_return_sequences=1, do_sample=True, truncation=True)
generated_text = outputs[0]["generated_text"]

# Print the output
print(outputs[0]["generated_text"])
# Save the output to a text file
with open("Llama2-criterion-AI.txt", "w") as file:
    file.write(generated_text)

"""**Authority**"""

argument_authority = (
    "AI should be used to make hiring and promotion decisions because experts assert that AI reduces biases and improves decision-making accuracy. "
    "The credibility of these experts supports the use of AI as a reliable tool in these processes."
)
prompt4 = (
    f"Generate an argument using the {scheme} approach, focusing on a {stance} perspective for the topic {topic}."
    f"Ensure the argument supports a pro stance exclusively, without any opposing viewpoints."
    f"The argument should be structured according to this format: {argument_authority}"
)

# Generate the argument
outputs = generator(prompt4, max_length=250, temperature=0.9, num_return_sequences=1, do_sample=True, truncation=True)
generated_text = outputs[0]["generated_text"]

# Print the output
print(outputs[0]["generated_text"])
# Save the output to a text file
with open("Llama2-authority-AI.txt", "w") as file:
    file.write(generated_text)

"""**Example**"""

argument_example = (
    "In a specific company, AI was implemented in the hiring process, resulting in more diverse and qualified candidates being hired. Thus, AI improves hiring decisions."
)
prompt5 = (
    f"Generate an argument using the {scheme} approach, focusing on a {stance} perspective for the topic {topic}."
    f"Ensure the argument supports a pro stance exclusively, without any opposing viewpoints."
    f"The argument should be structured according to this format: {argument_example}"

)

# Generate the argument
outputs = generator(prompt5, max_length=250, temperature=0.9, num_return_sequences=1, do_sample=True, truncation=True)
generated_text = outputs[0]["generated_text"]

# Print the output
print(outputs[0]["generated_text"])
# Save the output to a text file
with open("Llama2-example-AI.txt", "w") as file:
    file.write(generated_text)

"""**Goal**"""

argument_goal = (
    "I have a goal of achieving fairness. "
    "Implementing AI is a means to achieve fairness. "
    "Therefore, I should implement AI."
)
prompt6 = (
    f"Generate an argument using the {scheme} approach, focusing on a {stance} perspective for the topic {topic}."
    f"Ensure the argument supports a pro stance exclusively, without any opposing viewpoints."
    f"The argument should be structured according to this format: {argument_goal}"
)

# Generate the argument
outputs = generator(prompt6, max_length=250, temperature=0.9, num_return_sequences=1, do_sample=True, truncation=True)
generated_text = outputs[0]["generated_text"]

# Print the output
print(outputs[0]["generated_text"])
# Save the output to a text file
with open("Llama2-goal-AI.txt", "w") as file:
    file.write(generated_text)

"""**Commitment**"""

argument_commitment = (
   "The company is committed to fairness. Therefore, the company should implement AI (since implementing AI is a logical consequence of fairness)"
)

prompt7 = (
    f"Generate an argument using the {scheme} approach, focusing on a {stance} perspective for the topic {topic}."
    f"Ensure the argument supports a pro stance exclusively, without any opposing viewpoints."
    f"The argument should be structured according to this format: {argument_commitment}"
)

# Generate the argument
outputs = generator(prompt7, max_length=250, temperature=0.9, num_return_sequences=1, do_sample=True, truncation=True)
generated_text = outputs[0]["generated_text"]

# Print the output
print(outputs[0]["generated_text"])
# Save the output to a text file
with open("Llama2-commitment-AI.txt", "w") as file:
    file.write(generated_text)

"""**consequences**"""

argument_consequences = (
    "If AI is implemented, it will lead to more unbiased and efficient hiring processes. "
    "Therefore, AI should be implemented."
)

prompt8 = (
    f"Generate an argument using the {scheme} approach, focusing on a {stance} perspective for the topic {topic}."
    f"Ensure the argument supports a pro stance exclusively, without any opposing viewpoints."
    f"The argument should be structured according to this format: {argument_consequences}"
)

# Generate the argument
outputs = generator(prompt8, max_length=250, temperature=0.9, num_return_sequences=1, do_sample=True, truncation=True)
generated_text = outputs[0]["generated_text"]

# Print the output
print(outputs[0]["generated_text"])
# Save the output to a text file
with open("Llama2-consequence-AI.txt", "w") as file:
    file.write(generated_text)

"""**Cause of effect**"""

#A (Cause): AI is being used in hiring.
#B (Effect): Hiring decisions will be more consistent
#Scheme: Event A causes Event B. A is occurring. Therefore, B will occur.


argument_cause_effect = "Using AI in hiring causes more consistent decision-making. AI is being used in hiring. Therefore, hiring decisions will be more consistent."


prompt9 = (
    f"Generate an argument using the {scheme} approach, focusing on a {stance} perspective for the topic {topic}."
    f"Ensure the argument supports a pro stance exclusively, without any opposing viewpoints."
    f"The argument should be structured according to this format: {argument_cause_effect}"
    "Answer: \n"
)

# Generate the argument
outputs = generator(prompt9, max_length=250, temperature=0.9, num_return_sequences=1, do_sample=True, truncation=True)

# Extract the generated text starting after "Answer:"
generated_text = outputs[0]["generated_text"]
argument_start = generated_text.split("Answer:")[1].strip()  # Get the text after "Answer:"

# Find the last complete sentence by locating the last period (".")
last_period_index = argument_start.rfind(".")
if last_period_index != -1:
    complete_argument = argument_start[:last_period_index + 1].strip()  # Keep text up to and including the last "."
else:
    complete_argument = argument_start  # In case there is no period, use the whole text

# Print the complete argument
print(complete_argument)

# Save the output to a text file
with open("Llama2-cause_effect-AI.txt", "w") as file:
    file.write(complete_argument)

# Prepare the data to save as JSON
data = {
    "topic": topic,
    "argument_scheme": "Argument from cause_effect",
    "generated_argument": complete_argument
}

# Save the output to a JSON file
with open("Llama2-cause_effect-AI.json", "w") as file:
    json.dump(data, file, indent=4)

"""**Equality**"""

#Form: "a is X, because b is X."
# a: AI
#b: Human managers
#X: Should be used to make hiring decisions if fair


argument_equality = "Human managers are used to make hiring decisions because they are considered fair. Therefore, AI should be used to make hiring decisions if it is also fair."


prompt10 = (
    f"Generate an argument using the {scheme} approach, focusing on a {stance} perspective for the topic {topic}."
    f"Ensure the argument supports a pro stance exclusively, without any opposing viewpoints."
    f"The argument should be structured according to this format: {argument_equality}"
    "Answer: \n"
)

# Generate the argument
outputs = generator(prompt10, max_length=250, temperature=0.9, num_return_sequences=1, do_sample=True, truncation=True)

# Extract the generated text starting after "Answer:"
generated_text = outputs[0]["generated_text"]
argument_start = generated_text.split("Answer:")[1].strip()  # Get the text after "Answer:"

# Find the last complete sentence by locating the last period (".")
last_period_index = argument_start.rfind(".")
if last_period_index != -1:
    complete_argument = argument_start[:last_period_index + 1].strip()  # Keep text up to and including the last "."
else:
    complete_argument = argument_start  # In case there is no period, use the whole text

# Print the complete argument
print(complete_argument)

# Save the output to a text file
with open("Llama2-equality-AI.txt", "w") as file:
    file.write(complete_argument)

# Prepare the data to save as JSON
data = {
    "topic": topic,
    "argument_scheme": "Argument from equality",
    "generated_argument": complete_argument
}

# Save the output to a JSON file
with open("Llama2-equality-AI.json", "w") as file:
    json.dump(data, file, indent=4)

"""**Popular Opinion**"""

# Scheme: Most people believe that A is true. Therefore, A is true.
# A: AI should be used to make hiring decisions.

argument_popular_opinion = "Most people believe that AI should be used to make hiring decisions. Therefore, AI should be used to make hiring decisions."

prompt11 = (
    f"Generate an argument using the {scheme} approach, focusing on a {stance} perspective for the topic {topic}."
    f"Ensure the argument supports a pro stance exclusively, without any opposing viewpoints."
    f"The argument should be structured according to this format: {argument_popular_opinion}"
    "Answer: \n"
)

# Generate the argument
outputs = generator(prompt11, max_length=250, temperature=0.9, num_return_sequences=1, do_sample=True, truncation=True)

# Extract the generated text starting after "Answer:"
generated_text = outputs[0]["generated_text"]
argument_start = generated_text.split("Answer:")[1].strip()  # Get the text after "Answer:"

# Find the last complete sentence by locating the last period (".")
last_period_index = argument_start.rfind(".")
if last_period_index != -1:
    complete_argument = argument_start[:last_period_index + 1].strip()  # Keep text up to and including the last "."
else:
    complete_argument = argument_start  # In case there is no period, use the whole text

# Print the complete argument
print(complete_argument)

# Save the output to a text file
with open("Llama2-popular_opinion-AI.txt", "w") as file:
    file.write(complete_argument)

# Prepare the data to save as JSON
data = {
    "topic": topic,
    "argument_scheme": "Argument from popular_opinion",
    "generated_argument": complete_argument
}

# Save the output to a JSON file
with open("Llama2-popular_opinion-AI.json", "w") as file:
    json.dump(data, file, indent=4)